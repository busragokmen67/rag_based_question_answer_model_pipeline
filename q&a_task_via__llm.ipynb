{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AaLx6aRW4Wx",
        "outputId": "28a097d6-8737-4d4f-a755-54ff8988a153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Installations"
      ],
      "metadata": {
        "id": "hxKVRFjQTszZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers scikit-learn --quiet\n"
      ],
      "metadata": {
        "id": "0XuhJSw0Twp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & Preprocess .txt Files"
      ],
      "metadata": {
        "id": "uDHsolkhT5fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to your folder\n",
        "folder_path = '/content/drive/My Drive/q&a_task'\n",
        "\n",
        "all_files = []\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        full_path = os.path.join(root, file)\n",
        "        all_files.append(full_path)\n",
        "\n",
        "# Step 4: Print the list of all files\n",
        "print(f\"Found {len(all_files)} files.\")\n",
        "for f in all_files:\n",
        "    print(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meg25DRnYwxS",
        "outputId": "68da9228-2138-42c1-ad6e-f67f77361e50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12 files.\n",
            "/content/drive/My Drive/q&a_task/api.txt\n",
            "/content/drive/My Drive/q&a_task/authentication.txt\n",
            "/content/drive/My Drive/q&a_task/configuration.txt\n",
            "/content/drive/My Drive/q&a_task/error_codes.txt\n",
            "/content/drive/My Drive/q&a_task/export_logs.txt\n",
            "/content/drive/My Drive/q&a_task/faq.txt\n",
            "/content/drive/My Drive/q&a_task/installation.txt\n",
            "/content/drive/My Drive/q&a_task/intro.txt\n",
            "/content/drive/My Drive/q&a_task/reset_password.txt\n",
            "/content/drive/My Drive/q&a_task/usage.txt\n",
            "/content/drive/My Drive/q&a_task/user_roles.txt\n",
            "/content/drive/My Drive/q&a_task/webhooks.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load all text files from the specified directory\n",
        "def load_text_files(directory):\n",
        "    text_data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text_data.append(file.read())\n",
        "    return text_data\n",
        "\n",
        "directory = '/content/drive/My Drive/q&a_task'\n",
        "texts = load_text_files(directory)\n",
        "texts\n"
      ],
      "metadata": {
        "id": "TC3yddWlYyHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f760377e-bc6d-4975-ba5d-0e9628314e1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The HTTP API supports three endpoints:\\nPOST /api/index   ‚Üí build or rebuild the index\\nPOST /api/query   ‚Üí payload: { question, top_k }\\nGET  /api/status  ‚Üí returns { ready: true } when the index is ready\\n',\n",
              " 'Before running, set your API keys as environment variables.\\nUse OPENAI_API_KEY for OpenAI models and LLAMA_API_KEY for hosted Llama endpoints.\\nTo use a local embedding model, adjust settings in config/settings.py.\\nNever commit your API keys to version control.\\n',\n",
              " 'All settings live in config/settings.py:\\nEMBEDDING_MODEL, LLM_MODEL, VECTOR_STORE, CHUNK_SIZE, TOP_K.\\nTo override defaults, export the corresponding environment variables.\\nRestart the application after making any changes.\\n',\n",
              " '1001: Configuration file not found\\n1002: Missing API key\\n2001: Vector store unreachable\\n3001: LLM generation timed out\\n',\n",
              " 'To export past queries to CSV, run:\\n  python export_logs.py --output logs.csv\\nThe CSV includes timestamp, question, answer, and sources.\\nCustomize fields by editing export_logs.py before running.\\n',\n",
              " 'Can I use a local LLM?\\nYes‚Äîset LLM_PROVIDER to local in config/settings.py and install your model.\\n\\nHow do I update documents?\\nEdit files in the docs folder, then rerun the index command.\\n\\nWhy are answers incomplete?\\nTry increasing TOP_K or chunk size in settings.\\n\\nFor more, check the issue tracker or contact support.\\n',\n",
              " 'Step 1: Clone the repo (git clone https://‚Ä¶ && cd docuqa).\\nStep 2: Create and activate a virtual environment:\\n  python3 -m venv venv\\n  source venv/bin/activate\\nStep 3: Install dependencies:\\n  pip install -r requirements.txt\\nRequires Python 3.8 or higher.\\n',\n",
              " 'Welcome to DocuQA, your internal document Q&A assistant.\\nThis tool lets you query project documentation using a retrieval-augmented generation pipeline.\\nIt finds relevant text via embeddings, then uses an LLM to craft concise answers.\\nDrop your plain-text files into the docs folder to get started.\\n',\n",
              " 'To reset your password, open Settings and choose Security.\\nClick \"Forgot Password\" and enter your registered email address.\\nCheck your inbox for the reset link and follow the instructions.\\nIf no email arrives within five minutes, check spam or contact support.\\n',\n",
              " 'Invoke the CLI with:\\n  python qa.py --question \"Your question here\"\\nUse --top_k to adjust how many passages are retrieved.\\nEnter interactive mode with:\\n  python qa.py --interactive\\n',\n",
              " 'DocuQA defines three roles with distinct permissions:\\nAdmin can manage users, view logs, and update settings.\\nEditor can create and edit documentation pages.\\nViewer can read documents and ask questions but cannot modify content.\\n',\n",
              " 'Webhooks notify external services on index rebuild events.\\nConfigure event URLs in config/webhooks.yaml under ‚Äúindex_rebuilt‚Äù.\\nDocuQA POSTs a JSON payload with event and timestamp.\\nRetries occur up to three times with exponential backoff.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Preprocess text by removing unnecessary whitespace, special characters, and normalizing line breaks\n",
        "def preprocess_text(text):\n",
        "    # Remove leading/trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Optional: Remove non-text characters (e.g., special symbols or punctuation) if needed\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Uncomment if you want to remove punctuation\n",
        "\n",
        "    # Normalize line breaks (convert any \\n or \\r\\n to a single space)\n",
        "    text = re.sub(r'[\\n\\r]+', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load and preprocess all text files in the directory\n",
        "def load_and_preprocess_text_files(directory):\n",
        "    text_data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                raw_text = file.read()\n",
        "                cleaned_text = preprocess_text(raw_text)\n",
        "                text_data.append(cleaned_text)\n",
        "    return text_data\n",
        "\n",
        "# Load and preprocess the text files\n",
        "directory = '/content/drive/My Drive/q&a_task' # replace with your directory path\n",
        "texts = load_and_preprocess_text_files(directory)\n",
        "\n",
        "# Check the cleaned data (first 5 texts as example)\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"Cleaned Text {i+1}: {text[:300]}...\")  # Display first 300 characters of each cleaned file\n"
      ],
      "metadata": {
        "id": "Qz4RQeovYyJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8eac106-94f3-41d3-cfdb-546cf5e2b105"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text 1: The HTTP API supports three endpoints POST apiindex  build or rebuild the index POST apiquery  payload  question top_k  GET apistatus  returns  ready true  when the index is ready...\n",
            "Cleaned Text 2: Before running set your API keys as environment variables Use OPENAI_API_KEY for OpenAI models and LLAMA_API_KEY for hosted Llama endpoints To use a local embedding model adjust settings in configsettingspy Never commit your API keys to version control...\n",
            "Cleaned Text 3: All settings live in configsettingspy EMBEDDING_MODEL LLM_MODEL VECTOR_STORE CHUNK_SIZE TOP_K To override defaults export the corresponding environment variables Restart the application after making any changes...\n",
            "Cleaned Text 4: 1001 Configuration file not found 1002 Missing API key 2001 Vector store unreachable 3001 LLM generation timed out...\n",
            "Cleaned Text 5: To export past queries to CSV run python export_logspy output logscsv The CSV includes timestamp question answer and sources Customize fields by editing export_logspy before running...\n",
            "Cleaned Text 6: Can I use a local LLM Yesset LLM_PROVIDER to local in configsettingspy and install your model How do I update documents Edit files in the docs folder then rerun the index command Why are answers incomplete Try increasing TOP_K or chunk size in settings For more check the issue tracker or contact sup...\n",
            "Cleaned Text 7: Step 1 Clone the repo git clone https  cd docuqa Step 2 Create and activate a virtual environment python3 m venv venv source venvbinactivate Step 3 Install dependencies pip install r requirementstxt Requires Python 38 or higher...\n",
            "Cleaned Text 8: Welcome to DocuQA your internal document QA assistant This tool lets you query project documentation using a retrievalaugmented generation pipeline It finds relevant text via embeddings then uses an LLM to craft concise answers Drop your plaintext files into the docs folder to get started...\n",
            "Cleaned Text 9: To reset your password open Settings and choose Security Click Forgot Password and enter your registered email address Check your inbox for the reset link and follow the instructions If no email arrives within five minutes check spam or contact support...\n",
            "Cleaned Text 10: Invoke the CLI with python qapy question Your question here Use top_k to adjust how many passages are retrieved Enter interactive mode with python qapy interactive...\n",
            "Cleaned Text 11: DocuQA defines three roles with distinct permissions Admin can manage users view logs and update settings Editor can create and edit documentation pages Viewer can read documents and ask questions but cannot modify content...\n",
            "Cleaned Text 12: Webhooks notify external services on index rebuild events Configure event URLs in configwebhooksyaml under index_rebuilt DocuQA POSTs a JSON payload with event and timestamp Retries occur up to three times with exponential backoff...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Split Texts into Chunks\n",
        "\n",
        "Chunks are created with the minimum word count found in any file. This ensures even small files get their own chunk."
      ],
      "metadata": {
        "id": "VLrDYccbUBI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_min_file_word_count(texts):\n",
        "    # Calculate the word count for each file and return the minimum word count\n",
        "    min_word_count = float('inf')  # Start with a very large number\n",
        "\n",
        "    for text in texts:\n",
        "        word_count = len(text.split())\n",
        "        if word_count < min_word_count:\n",
        "            min_word_count = word_count\n",
        "\n",
        "    return min_word_count\n",
        "\n",
        "def split_text_into_chunks_based_on_min_file(texts):\n",
        "    # Calculate the minimum word count based on the smallest file\n",
        "    min_words_per_chunk = calculate_min_file_word_count(texts)\n",
        "    print(f\"Minimum Words per Chunk: {min_words_per_chunk}\")  # Debugging line to check min count\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            current_chunk.append(word)\n",
        "\n",
        "            # When current chunk reaches the minimum word count, finalize the chunk and start a new one\n",
        "            if len(current_chunk) >= min_words_per_chunk:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = []  # Start a new chunk\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Apply the chunking logic based on the smallest file word count\n",
        "chunks = split_text_into_chunks_based_on_min_file(texts)\n",
        "\n",
        "# Check the result\n",
        "print(f\"Total Chunks: {len(chunks)}\")\n",
        "for idx, chunk in enumerate(chunks):  # Display the first 5 chunks for inspection\n",
        "    print(f\"Chunk {idx+1}: {chunk[:300]}...\")  # Display the first 300 characters of each chunk\n"
      ],
      "metadata": {
        "id": "ejAu-J9cYyML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5351c83b-ff98-4d24-d3cf-ea896310c6be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Words per Chunk: 18\n",
            "Total Chunks: 22\n",
            "Chunk 1: The HTTP API supports three endpoints POST apiindex build or rebuild the index POST apiquery payload question top_k...\n",
            "Chunk 2: GET apistatus returns ready true when the index is ready Before running set your API keys as environment...\n",
            "Chunk 3: variables Use OPENAI_API_KEY for OpenAI models and LLAMA_API_KEY for hosted Llama endpoints To use a local embedding model...\n",
            "Chunk 4: adjust settings in configsettingspy Never commit your API keys to version control All settings live in configsettingspy EMBEDDING_MODEL...\n",
            "Chunk 5: LLM_MODEL VECTOR_STORE CHUNK_SIZE TOP_K To override defaults export the corresponding environment variables Restart the application after making any...\n",
            "Chunk 6: changes 1001 Configuration file not found 1002 Missing API key 2001 Vector store unreachable 3001 LLM generation timed...\n",
            "Chunk 7: out To export past queries to CSV run python export_logspy output logscsv The CSV includes timestamp question answer...\n",
            "Chunk 8: and sources Customize fields by editing export_logspy before running Can I use a local LLM Yesset LLM_PROVIDER to...\n",
            "Chunk 9: local in configsettingspy and install your model How do I update documents Edit files in the docs folder...\n",
            "Chunk 10: then rerun the index command Why are answers incomplete Try increasing TOP_K or chunk size in settings For...\n",
            "Chunk 11: more check the issue tracker or contact support Step 1 Clone the repo git clone https cd docuqa...\n",
            "Chunk 12: Step 2 Create and activate a virtual environment python3 m venv venv source venvbinactivate Step 3 Install dependencies...\n",
            "Chunk 13: pip install r requirementstxt Requires Python 38 or higher Welcome to DocuQA your internal document QA assistant This...\n",
            "Chunk 14: tool lets you query project documentation using a retrievalaugmented generation pipeline It finds relevant text via embeddings then...\n",
            "Chunk 15: uses an LLM to craft concise answers Drop your plaintext files into the docs folder to get started...\n",
            "Chunk 16: To reset your password open Settings and choose Security Click Forgot Password and enter your registered email address...\n",
            "Chunk 17: Check your inbox for the reset link and follow the instructions If no email arrives within five minutes...\n",
            "Chunk 18: check spam or contact support Invoke the CLI with python qapy question Your question here Use top_k to...\n",
            "Chunk 19: adjust how many passages are retrieved Enter interactive mode with python qapy interactive DocuQA defines three roles with...\n",
            "Chunk 20: distinct permissions Admin can manage users view logs and update settings Editor can create and edit documentation pages...\n",
            "Chunk 21: Viewer can read documents and ask questions but cannot modify content Webhooks notify external services on index rebuild...\n",
            "Chunk 22: events Configure event URLs in configwebhooksyaml under index_rebuilt DocuQA POSTs a JSON payload with event and timestamp Retries...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Generate Embeddings with Sentence-BERT\n",
        "\n",
        "Use the all-MiniLM-L6-v2 model ‚Äî small and fast."
      ],
      "metadata": {
        "id": "IFgkFaccUG5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the Sentence-BERT model\n",
        "e_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Check if the model is loaded properly\n",
        "print(\"Sentence-BERT Model Loaded!\")\n"
      ],
      "metadata": {
        "id": "tf7U-qSTYyRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74406f74-a5f2-4d23-ffa2-99ce8c655851"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence-BERT Model Loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings = e_model.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "\n",
        "print(f\"Generated Embeddings: {embeddings.shape}\")\n"
      ],
      "metadata": {
        "id": "MKr-sfdXYwz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7d939f-c4a5-47a2-be37-15a4bb966b04"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Embeddings: torch.Size([22, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Retrieve Relevant Chunks with Cosine Similarity"
      ],
      "metadata": {
        "id": "WbgC3F6GUNkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def retrieve_top_k_chunks_cosine(query, model, embeddings, chunks, k=3):\n",
        "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
        "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
        "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
        "    top_chunks = [chunks[i] for i in top_k_indices]\n",
        "    return top_chunks, similarities[top_k_indices]\n"
      ],
      "metadata": {
        "id": "hd55eQ0z8bPN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Generate Answer with Falcon-1B(LLM)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Falcon is light and works on free Colab without needing an API."
      ],
      "metadata": {
        "id": "UjHApZxSUqEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_directly(query, top_chunks):\n",
        "    context = \"\\n\\n\".join(top_chunks)\n",
        "    prompt = f\"\"\"Use the following context to answer the question:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "    result = generator(prompt)[0][\"generated_text\"]\n",
        "    return result.split(\"Answer:\")[-1].strip()\n"
      ],
      "metadata": {
        "id": "SmCnC2gHCfyk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Integration: Query, Retrieve, and Generate Answer"
      ],
      "metadata": {
        "id": "1VkVT_yxUmHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"tiiuae/falcon-rw-1b\"  # Light model, free and usable on CPU\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "def answer_question(query):\n",
        "    context = \"\\n\\n\".join(retrieve_chunks(query))\n",
        "    prompt = f\"\"\"\n",
        "Use the following context to answer the question:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "    result = generator(prompt)[0][\"generated_text\"]\n",
        "    return result.split(\"Answer:\")[-1].strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "collapsed": true,
        "id": "w6Aldt28zh2D",
        "outputId": "a1114e30-1bb1-465d-cf3d-b14c3cc45c1e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-588730976.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5174\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5175\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5176\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5177\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5441\u001b[0m             original_checkpoint_keys = list(\n\u001b[0;32m-> 5442\u001b[0;31m                 \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5443\u001b[0m             )\n\u001b[1;32m   5444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"meta\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mmap\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2117\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2120\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m                         \u001b[0;34mf\"Only persistent_load of storage is allowed, but got {pid[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                     )\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_BINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2081\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   2084\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2034\u001b[0m                     )\n\u001b[1;32m   2035\u001b[0m             storage = (\n\u001b[0;32m-> 2036\u001b[0;31m                 \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2037\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How do I reset my password?\"\n",
        "top_chunks, _ = retrieve_top_k_chunks_cosine(query, e_model, embeddings, chunks, k=2)\n",
        "\n",
        "print(\"Most relevant answer:\", top_chunks[0])\n",
        "print(\"Second relevant answer:\", top_chunks[1])\n",
        "\n",
        "answer = generate_answer_directly(query, top_chunks)\n",
        "\n",
        "print(\"üîç Question:\", query)\n",
        "print(\"üß† Answer:\", answer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc6XyCtd6dvQ",
        "outputId": "60ea4b40-90c7-4b84-cb9e-a4a11c02e4c9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant answer: To reset your password open Settings and choose Security Click Forgot Password and enter your registered email address\n",
            "Second relevant answer: Check your inbox for the reset link and follow the instructions If no email arrives within five minutes\n",
            "üîç Question: How do I reset my password?\n",
            "üß† Answer: Changing your password will ensure you are the only person who can access your account, and will help improve your security..\n",
            "This post originally appeared on the author‚Äôs blog\n"
          ]
        }
      ]
    }
  ]
}